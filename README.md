# ğŸ¤– miniRAG - Enterprise RAG AI Assistant

An advanced **Retrieval-Augmented Generation (RAG)** system with enterprise-grade UI, file upload capabilities, calculated confidence scoring, and comprehensive performance metrics.

## ğŸ”— Live Demo

Demo URL:  
https://minirag-1c3t.onrender.com



---

## âœ¨ Key Features

- ğŸ“ **File Upload Support**: PDF and TXT document processing with automatic text extraction
- ğŸ¯ **Calculated Confidence**: Percentage-based confidence scoring (0-99%) with color-coded indicators
- ğŸ“Š **Comprehensive Metrics**: Detailed performance tracking including:
  - Retrieval time
  - Reranking time
  - LLM processing time
  - Total response time
  - Token usage
  - Estimated cost
- ğŸ¨ **Enterprise UI**: Professional dark/light theme with animated gradient backgrounds
- ğŸ” **Smart Search**: Vector-based semantic search with Qdrant cloud integration
- ğŸ¤– **AI-Powered**: Groq LLM for intelligent question answering
- ğŸ“ **Optional Metadata**: Flexible source field with auto-fill functionality

---

## ğŸ—ï¸ Project Structure

```
Mini-Rag-Policy-QA/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                    # FastAPI main application
â”‚   â”œâ”€â”€ answer_generator.py       # LLM response generation
â”‚   â”œâ”€â”€ answer.py                 # Answer processing logic
â”‚   â”œâ”€â”€ chunking.py               # Document chunking utilities
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”œâ”€â”€ context_builder.py        # Context preparation for LLM
â”‚   â”œâ”€â”€ embeddings.py             # Text embedding generation
â”‚   â”œâ”€â”€ ingest.py                 # Document ingestion pipeline
â”‚   â”œâ”€â”€ qdrant_conn.py            # Qdrant database connection
â”‚   â”œâ”€â”€ rerank.py                 # Result reranking logic
â”‚   â”œâ”€â”€ retrieve.py               # Vector search retrieval
â”‚   â”œâ”€â”€ schemas.py                # Pydantic data models
â”‚   â”œâ”€â”€ vector_store.py           # Vector storage operations
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ index.html            # Enterprise-grade frontend UI
â”œâ”€â”€ eval/                         # Evaluation scripts
â”œâ”€â”€ .env                          # Environment variables (API keys)
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ runtime.txt                   # Python version specification
â””â”€â”€ README.md                     # Project documentation
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12.5+
- Qdrant Cloud account (or local Qdrant instance)
- Groq API key

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/anish-143/miniRAG.git
cd miniRAG
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment
Create a `.env` file with:
```env
LLM_API_KEY=your_groq_api_key_here
QDRANT_URL=your_qdrant_cloud_url_here
QDRANT_API_KEY=your_qdrant_api_key_here
```

### 5ï¸âƒ£ Run the Application
```bash
uvicorn backend.app:app --host 0.0.0.0 --port 8000 --reload
```

### 6ï¸âƒ£ Access the UI
Open your browser at: **http://localhost:8000/ui/**

---

## ğŸ“¡ API Endpoints

| Method | Endpoint   | Description                          |
|--------|-----------|--------------------------------------|
| GET    | `/health` | Health check                         |
| POST   | `/ingest` | Ingest document (text + metadata)    |
| POST   | `/upload` | Upload PDF/TXT file                  |
| POST   | `/query`  | Ask questions about documents        |
| GET    | `/ui/`    | Access web interface                 |

---

## ğŸ¯ How It Works

### Document Ingestion Flow
1. **Upload**: User uploads PDF/TXT file or inputs text manually
2. **Chunking**: Document split into overlapping chunks (~800 tokens, 100 token overlap)
3. **Embedding**: Chunks converted to vector embeddings
4. **Storage**: Vectors stored in Qdrant cloud database with metadata

### Query Processing Flow
1. **Query Embedding**: User question converted to vector
2. **Retrieval**: Top-K similar chunks retrieved from Qdrant
3. **Reranking**: Results reranked by relevance
4. **Context Building**: Selected chunks assembled as context
5. **LLM Generation**: Groq generates answer from context
6. **Metrics Calculation**: Performance metrics computed
7. **Confidence Scoring**: Percentage-based confidence calculated
8. **Response**: Answer with metrics, confidence, and sources returned

---

## ğŸ“Š Performance Metrics

The system tracks and displays:
- **Retrieval Time**: Vector search duration (ms)
- **Reranking Time**: Result reranking duration (ms)
- **LLM Time**: AI processing duration (ms)
- **Total Time**: End-to-end response time (ms)
- **Tokens Used**: Estimated token count
- **Estimated Cost**: Calculated based on Groq pricing ($0.05/1M tokens)

---

## ğŸ¨ UI Features

- **Dark/Light Theme**: Toggle between themes
- **Animated Background**: Gradient with floating orbs
- **File Upload**: Drag-and-drop or click to upload
- **Optional Source**: Auto-fills with document title
- **Confidence Indicators**: Color-coded percentage scores
  - ğŸŸ¢ High (85-99%): Green
  - ğŸŸ¡ Medium (40-84%): Yellow
  - ğŸ”´ Low (0-39%): Red
- **Metrics Dashboard**: Real-time performance statistics
- **Responsive Design**: Works on desktop and mobile

---

## ğŸ”§ Technology Stack

### Backend
- **FastAPI**: Modern Python web framework
- **Qdrant**: Vector database for semantic search
- **Groq**: High-performance LLM API
- **pypdf**: PDF text extraction
- **sentence-transformers**: Text embeddings

### Frontend
- **HTML5/CSS3/JavaScript**: Pure vanilla stack
- **Font Awesome**: Icon library
- **Google Fonts**: Space Grotesk, Inter, JetBrains Mono

---

## âš™ï¸ Configuration

### Deployment Modes
- **Full Mode**: Uses sentence-transformers and cross-encoder reranking
- **Lite Mode**: Mock embeddings for memory-constrained environments

Set in `.env`:
```env
DEPLOYMENT_MODE=full  # or 'lite'
```

---

## ğŸ§ª Example Usage

### Upload a Document
1. Click "Upload Document"
2. Select PDF/TXT file or paste text
3. Enter title (required)
4. Enter source (optional - auto-fills with title)
5. Click "Ingest"

### Ask Questions
1. Type your question in the query box
2. Click "Ask"
3. View:
   - AI-generated answer
   - Confidence score
   - Performance metrics
   - Retrieved sources

---

## ğŸ“ Example Queries

- "What is blockchain technology?"
- "Explain the consensus mechanism"
- "How does distributed ledger work?"
- "What are smart contracts?"

---

## ğŸ‘¤ Author

**Name:** Anish Kumar  
**Roll No:** 231210016  
**Branch:** Computer Science & Engineering (CSE)  
**College:** National Institute of Technology (NIT) Delhi  
**Email:** 231210016@nitdelhi.ac.in  
**GitHub:** https://github.com/anish-143  
**Resume:** https://drive.google.com/file/d/1-z8Fygk5TdMhaqnXgXh6coy4UCvva60y/view?usp=drive_link

## ğŸ“„ License

This project is created for academic purposes at NIT Delhi.

---

## ğŸ™ Acknowledgments

- NIT Delhi CSE Department
- Qdrant for vector database technology
- Groq for LLM API access
- FastAPI for the excellent framework  


